\section{Распределённое машинное обучение. Разделение градиента, алгоритм с
  обменом градиентами, проблемы при масштабировании.}

\begin{algorithm}(Разбиение градиента)
  \begin{itemize}
    \item Датасет разбивается на части.

      $$D_i \subset D$$

      Определим функцию потерь на части датасета.

      $$\mathbb{L}_{D_i} = \sum_{x, y \in D_i} L(x, y, \hat{y})$$

      Тогда значение функции потерь на всем датасете есть сумма значений
      функции потерь на частях датасета.

      $$\mathbb{L} = \sum_{x, y \in D} L(x, y, \hat{y})
        = \sum_i \sum_{x, y \in D_i} L(x, y, \hat{y})
        = \sum_i \mathbb{L}_{D_i}$$

      Получаем, что значение частных производных по параметру есть сумма
      частных производных по параметру на каждой части датасета.

      $$\frac{\partial \mathbb{L}}{\partial W}
        = \frac{\partial \left( \mathlarger{\sum_i} \mathbb{L}_{D_i} \right)}
               {\partial W}
        = \sum_i \frac{\partial \mathbb{L}_{D_i}}{\partial W}$$
    \item Каждый узел считает градиент на своей части датасета, после чего
      рассылает его остальным узлам.
    \item На каждом узле полученные градиенты суммируются с собственными.
    \item Итоговый градиент используется в оптимизаторе.
  \end{itemize}
\end{algorithm}

\begin{remark} Данный алгоритм работает в синхронной сети, без сбоев узлов.
\end{remark}

\textbf{Масштабирование.}
\begin{itemize}
  \item Для рассылки градиента каждый узел посылает $N (N - 1) \abs{W}$ байт.
  \item Оценка сложности вычисления градиента и обновления весов: $\mathcal{O}
    \left( \abs{W} \frac{\mathlarger{|D|}}{\mathlarger{N}} \right)$.
  \item Оценка сложности пересылки градиента: $\mathcal{O} \left( \abs{W} N
    \right)$.
\end{itemize}

\begin{remark} При использовании данного подхода увеличение числа узлов приводит
  к увеличению доли времени исполнения, затрачиваемого на пересылку градиента.
  Способы борьбы с этим рассматриваются в следующих билетах.
\end{remark}
