\section{Распределённое машинное обучение. Quantization, Sparsification,
  Error Correction.}

\begin{algorithm}(1-Bit Quantization)
  \begin{itemize}
    \item Посчитаем средние значения среди положительных и
      отрицательных компонент градиента, обозначим за $\mathbb{E}_+$ и $\mathbb{E}_-$
      соответственно.
    \item Для каждой компоненты градиента будем посылать один бит, отвечающий
      за знак исходного значения.
    \item Также будем посылать два числа: $\mathbb{E}_+$ и $\mathbb{E}_-$.
    \item На принимающей стороне заменим все положительные компоненты градиента на $\mathbb{E}_+$,
      а отрицательные на $\mathbb{E}_-$.
    \item Размер такого пересылаемого градиента будет $2 \cdot 32 + \abs{W}$ бит вместо
      $32 \cdot \abs{W}$ бит.
     \item После таких преобразований потеряли в точности (слабо) и в скорости сходимости (сильно).
  \end{itemize}
\end{algorithm}

\begin{algorithm}(Stochastic Quantization)
  \begin{itemize}
    \item Нормируем все компоненты градиента на отрезок $[-1, 1]$.

      $$\hat{W}_i
        = \frac{W_i}{\sqrt{\mathlarger{\sum_{j = 1}^{\abs{W}}}W_j^2}}
        \in [-1, 1]$$
    \item $k$ --- число бит, которыми будет кодироваться одна компонента.
      Разобьем отрезок $[-1, 1]$ на $2^k - 1$ равных подотрезков, сопоставив
      их границам числа от $0$ до $2^k - 1$.
    \item Для кодирования очередной компоненты градиента будем делать следующее:
      \begin{itemize}
        \item Выбираем подотрезок, на который попадает отнормированное значение;
        \item Пусть значение равно $w$, и оно попадает на подотрезок $[x, y]$.
          Подбросим нечестную монетку $f$ и выберем одну из границ. Вероятность
          выпадения каждой границы линейно зависит от расстояния $w$ до
          противоположной.

          $$\mathbb{P}(f(w) = \mathcal{E}) =
            \begin{cases}
              \frac{y - w}{y - x}, & \mathcal{E} = x \\
              \frac{w - x}{y - x}, & \mathcal{E} = y \\
            \end{cases}$$

          В зависимости от выбранной границы, кодируем компоненту числом от $0$
          до $2^k - 1$, соответствующим границе;
        \item Преобразование получилось несмещенным, повышает точность и скорость сходимости.

          $$\mathbb{E}(f(w)) = x \cdot \frac{y - w}{y - x} + y \cdot
            \frac{w - x}{y - x} = w$$.

      \end{itemize}
      \item Посылаем другим узлам $k$ чисел --- закодированный градиент, а так же нормировочную  константу.
      \item Размер такого градиента будет $32 + k \cdot \abs{W}$ бит вместо
        $32 \cdot \abs{W}$ бит.
  \end{itemize}
\end{algorithm}

\begin{algorithm}(Sparsification)
  \begin{itemize}
    \item В градиенте остаются только $k$ наибольших по модулю компоненты,
      остальные заменяются на нули.
    \item При пересылке отправляются только значения ненулевых компонент с
      их номерами.
    \item Размер такого градиента будет $32 \cdot k  + k \cdot \log{\abs{W}}$
      бит вместо $32 \cdot \abs{W}$ бит.
    \item Данный подход совместим с предыдущими алгоритмами.
  \end{itemize}
\end{algorithm}

\begin{algorithm}(Error correction)
  \begin{itemize}
    \item Пусть на шаге $i$ был градиент $\nabla_i$.
    \item После преобразований вышеописанных алгоритмов, получим преобразованный
      градиент $\tilde{\nabla}_i$.
    \item Сохраним локально ошибку $err_i = \nabla_i - \tilde{\nabla}_i$.
    \item На следующем шаге, скорректируем градиент с учетом этой ошибки:
      $\nabla_{i + 1} = \nabla_{i + 1} + err_i$.
  \end{itemize}
\end{algorithm}
