\section{Распределённое машинное обучение. Схемы пересылки сообщений, обмен
  весами, послойное обучение, SwarmSGD.}

\textbf{Схема пересылки сообщений: через мастер-узел.}
\begin{itemize}
  \item Каждый узел посылает свой градиент мастер-узлу, который просуммирует их
    локально.
  \item Затем каждый узел получает от мастер-узла сумму.
  \item Таким образом, суммирование произошло один раз. При этом число
    пересланных каждым узлом (кроме мастер-узла) бит равно
    $\mathcal{O}(\abs{W})$ вместо $\mathcal{O}(\abs{W} N)$. В системе будет
    послано $2  (N - 1)$ сообщений вместо $N (N - 1)$.
\end{itemize}

\textbf{Схема пересылки сообщений: по кругу.}
\begin{itemize}
  \item Сначала градиент пересылается от $1$ узла ко $2$ узлу, от $2$ узла
    к $3$, и так далее. При прохождении через каждый узел к пересылаемому
    градиенту прибавляется локальный градиент узла.
  \item Когда градиент обрабатывает узел $N$, в нем просуммированы все локальные
    градиенты каждого узла. Он рассылается всем узлам обратно (напр. от $N$ к
    $N - 1$, от $N - 1$ к $N - 2$ и так далее).
  \item В системе будет послано $2  (N - 1)$ сообщений вместо $N (N - 1)$.
  \item Алгоритм полностью децентрализован.
\end{itemize}

\textbf{Схема пересылки сообщений: Gossip.}
\begin{itemize}
  \item Каждый узел на каждом шаге посылает свой градиент случайному
    подмножеству других узлов.
  \item При получении информации от соседних узлов --- пересылает ее.
  \item Узел ждет, пока не получит градиент от всех остальных.
\end{itemize}

\begin{algorithm}(Обмен весами)
  \begin{itemize}
    \item Каждый узел независимо учит модель $K$ ходов, причем только на своих
      локальных данных.
    \item Затем все узлы обмениваются \textbf{весами} между собой и усредняют.
    \item При таком подходе посылается в $K$ раз меньше данных. Но при слишком
      большом значении параметра скорость схождения падает.
  \end{itemize}
\end{algorithm}

\begin{algorithm}(SwarmSGD)
  \begin{itemize}
    \item Случайно выбираются два узла, которые независимо параллельно
      учат модель на своих локальных данных $K$ ходов.
    \item Затем узлы обмениваются весами между собой и усредняют.
    \item При таком подходе нет глобального усреднения весов, но алгоритм
      сходится.
  \end{itemize}
\end{algorithm}

\begin{algorithm}(Послойное обучение)
  \begin{itemize}
    \item Слои нейронной сети разбиваются на $N$ групп, за кажду группу отвечает
      один узел.
    \item В течение $K$ шагов каждый узел учит исключительно свою группу слоев.
      При этом градиенты по более глубоким слоям также считаются, но не
      записываются, а в градиенте обнуляются.
    \item Затем узлы пересылают друг другу обновленные веса нейронов своих
      групп.
  \end{itemize}
\end{algorithm}
