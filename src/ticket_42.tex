\section{Multi-Leader репликация. Мотивация, схема с множеством дата-центров, конфликты при записи. Синхронное разрешение конфликтов, избегание конфликтов.}

\begin{definition}
    \textit{Multi-Leader репликация:}
    \begin{itemize}
        \item Можем писать в любого из лидеров $\Rightarrow$ уменьшаем задержку записи.
        \item Несколько лидеров принимают запросы на запись $\Rightarrow$ увеличиваем пропускную способность на запись.
        \item Можем пережить падение лидера и даже после сбоя сможем обслуживать запросы как на чтение, так и на запись $\Rightarrow$ увеличиваем надежность.

        \item \textbf{Общая схема:}
        \begin{itemize}
            \item Несколько дата-центров, в каждом --- свой лидер.
            \item У каждого лидера несколько реплик, подчинённых ему по схеме Leader/Follower.
            \item Лидер принимает запросы на чтение и запись, реплики --- только на чтение.
        \end{itemize}

        \item \textbf{Конфликты при записи:}
        \begin{itemize}
            \item Параллельные конфликтующие записи являются проблемой, даже если нет аппаратных сбоев.
            \item В случае аппаратных сбоев ситуация становится ещё сложнее, так как реплики не могут синхронизироваться и обнаружить конфликт.
        \end{itemize}
    \end{itemize}

\end{definition}

\begin{algorithm}(Синхронное разрешение конфликтов)
    \begin{itemize}
        \item Не говорим пользователю, что его операция завершена, пока она не отреплицируется на всех лидеров или хотя бы на кворум.
        \item Решение идейно эквивалентно Paxos/Raft/etc.
        \item Низкая доступность: изолированный лидер не сможет принимать запросы на запись, но по-другому быть не может вследствие CAP-теоремы.

        \item Высокая задержка записи: пользователь должен ждать, пока запись отреплицируется на кворум лидеров. Решение:
        \begin{itemize}
            \item Возвращать пользователю ответ сразу, как только хотя бы один лидер обслужил запись.
            \item Тогда задержка, обусловленная репликацией на других лидеров, будет скрыта от пользователя.
        \end{itemize}
    \end{itemize}

\end{algorithm}

\newpage

\begin{algorithm}(Избегание конфликтов)
    \begin{itemize}
        \item Если параллельные изменения объекта X могут привести к конфликту, будем направлять все запросы на изменение объекта X через одного лидера.
        \item На практике часто помогает уменьшить количество конфликтов.
        \item Есть много случаев, когда такая стратегия не применима.
        \item Проблемы:
        \begin{itemize}
            \item Пользователь может переехать, и запросы на изменение его объявлений должны пойти через нового ближайшего лидера.
            \item Не всегда все запросы на изменение объекта X можно направлять через одного лидера.
        \end{itemize}
    \end{itemize}

\end{algorithm}

\begin{algorithm}(Использование CRDT)
    \begin{itemize}
        \item Конфликтов не бывает в принципе: применение операций в любом порядке приводит к одному и тому же результату.
        \item С помощью Gossip когда-нибудь на каждый узел отреплицируется весь набор операций.
        \item Значит, итоговое значение на каждом узле будет одинаковым.
    \end{itemize}

\end{algorithm}
